# -*- coding: utf-8 -*-
"""machine_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Jv8M8zZfPdEa4jqU4zkmhSFs3muOZpU

# Exercice 1 "Perceptron simple"
"""

import numpy as np

x=np.array([[1,1,0,0],[0,1,0,1],[1,0,1,1]])
t=np.array([[0,1,0],[0,1,1],[1,1,0]])

w = np.random.uniform(-1,1, size=(4, 3))
w

o=np.zeros((3,3))
o

t.transpose()

def apprentissage(t,x,W,O):
  c = 0
  while (not(np.array_equal(t,O))):
    for i in range(np.shape(t)[0]):
      for j in range(np.shape(t)[1]):
        if (np.dot(x,W)[i,j]<0):
          O[i,j]=0
        else:
          O[i,j]=1
    W=W+0.3*np.dot(x.transpose(),(t-O))
    c+=1
  print("nombre d'itérations =",c)
  print(W)

apprentissage(t,x,w,o)

np.array_equal(t,o)

"""# Exercice2 "perceptron de conjugaison"
"""

l1 = [[1,0,0], [0,1,0], [0,0,1]]
l2 = [[1,0],[0,1]]

l=[]
for i in l1:
  for j in l1:
    for k in l2:
      li=[]
      li.append(i)
      li.append(j)
      li.append(k)
      l.append(li)
l

X=[]
for i in range(18):
  X.append(sum(l[i], []))

X

X=np.array(X)

T=X

aux=0
for i in range(1,18):
  aux=T[i][6]
  T[i][6]=T[i][7]
  T[i][7]=aux

T
T.transpose()

W = np.random.uniform(-1,1, size=(8,8))
O=np.zeros((18,8))

W

O

apprentissage(T,X,W,O)

np.array_equal(T,O)

"""# Exercice 3 : Perceptron probabiliste"""

import random
import math

def apprentissage_proba(t,x,W,O):
  c = 0
  print("matrice des poids init",W)
  while (not(np.array_equal(t,O))):
    for i in range(np.shape(t)[0]):
      for j in range(np.shape(t)[1]):
        p= 1 /( 1 + math.exp(-np.dot(x,W)[i,j]))
        r= random.random()
        if (p<r):
          O[i,j]=0
        else:
          O[i,j]=1
    W=W+0.3*np.dot(x.transpose(),(t-O))
    c+=1
  print("nombre d'itérations =",c)
  print("matrice des poids finale" ,W)

x2=np.array([[1,1,0,0],[0,1,0,1],[1,0,1,1]])
t2=np.array([[0,1,0],[0,1,1],[1,1,0]])
o2=np.zeros((3,3))
w2 = np.random.uniform(-1,1, size=(4, 3))

apprentissage_proba(t2,x2,w2,o2)

"""# Exercice4 : Apprentissage Par Retropropagation d'erreur"""

import numpy as np

x=np.array([[1],[2],[3]])
t=np.array([[0.1],[0.3],[0.7]])
W=np.array([[0.5,0.3,0.1],[0.3,0.2,0.1]]) #w = np.random.uniform(-1,1, size=(2, 3))
Z=np.array([[0.1,0.2],[0.3,0.4],[0.5,0.6]]) #z = np.random.uniform(-1,1, size=(3, 2))
epsilon=10**(-9) #précision
eta=1 #learning rate

def f(x) :
    return(1 / (1+np.exp(-x)))

# mise à jour de la matrice des weights
def maj(A,eta,d,m): 
    return(A+eta*np.dot(d,m.transpose()))

def apprentissage_retro_prop(x,t,W,Z,epsilon,eta) :
    c=0
    e=np.ones((3,1))
    o=np.zeros((3,1))
    while(np.linalg.norm(e)>epsilon):
        b=np.dot(W,x)
        h=f(b)
        a=np.dot(Z,h)
        o=f(a)
        e=t-o
        delta_sortie = e * o * (1 - o)
        Z=maj(Z,eta,delta_sortie,h)
        delta_cachée= h * (1 - h) * (np.dot(Z.transpose(),delta_sortie))
        W=maj(W,eta,delta_cachée,x)
        c +=1
    print("nombre d'itérations = ",c)
    print("erreur e \n", np.linalg.norm(e))

apprentissage_retro_prop(x,t,W,Z,epsilon,eta)


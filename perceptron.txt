import numpy as np

x=np.array([[1],[2],[3]])
t=np.array([[0.1],[0.3],[0.7]])
W=np.array([[0.5,0.3,0.1],[0.3,0.2,0.1]]) #w = np.random.uniform(-1,1, size=(2, 3))
Z=np.array([[0.1,0.2],[0.3,0.4],[0.5,0.6]]) #z = np.random.uniform(-1,1, size=(3, 2))
epsilon=10**(-9) #précision
eta=1 #learning rate

def f(x) :
    return(1 / (1+np.exp(-x)))

# mise à jour de la matrice des weights
def maj(A,eta,d,m): 
    return(A+eta*np.dot(d,m.transpose()))

def apprentissage_retro_prop(x,t,W,Z,epsilon,eta) :
    c=0
    e=np.ones((3,1))
    o=np.zeros((3,1))
    while(np.linalg.norm(e)>epsilon):
        b=np.dot(W,x)
        h=f(b)
        a=np.dot(Z,h)
        o=f(a)
        e=t-o
        delta_sortie = e * o * (1 - o)
        Z=maj(Z,eta,delta_sortie,h)
        delta_cachée= h * (1 - h) * (np.dot(Z.transpose(),delta_sortie))
        W=maj(W,eta,delta_cachée,x)
        c +=1
    print("nombre d'itérations = ",c)
    print("erreur e \n", np.linalg.norm(e))

apprentissage_retro_prop(x,t,W,Z,epsilon,eta)

